{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from img_scraper.model import db_connect,create_table\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from model import db_connect,create_table\n",
    "from bs4 import Tag\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "def grap_preview_imgs_urls(img_table, engine):\n",
    "    '''\n",
    "    get img - url pair from the model\n",
    "    db_img_http: column name from the model,such as preview_img_link/title_img_url\n",
    "    db_img_local:column name from the mode, such as preview_img_local/title_img_local\n",
    "    '''\n",
    "    ori_df = pd.read_sql_table(img_table, engine)\n",
    "    preview_imgs_urls = {}\n",
    "    for row in ori_df.itertuples():\n",
    "        #         print(dir(row))\n",
    "        img_url = {}\n",
    "        prview_img_links = []\n",
    "        preview_img_locals = []\n",
    "        prview_img_links.append(row.preview_img_link)\n",
    "        preview_img_locals.append(row.preview_img_local)\n",
    "        img_url['preview_img_link'] = prview_img_links\n",
    "        img_url['preview_img_local'] = preview_img_locals\n",
    "        if not preview_imgs_urls.get(row.orig_id):\n",
    "            preview_imgs_urls[row.orig_id] = img_url\n",
    "\n",
    "    return preview_imgs_urls\n",
    "\n",
    "\n",
    "def grap_content_imgs_urls(table, engine):\n",
    "    '''\n",
    "    get img - url pair from the model\n",
    "    db_img_http: column name from the model,such as preview_img_link/title_img_url\n",
    "    db_img_local:column name from the mode, such as preview_img_local/title_img_local\n",
    "    '''\n",
    "    ori_df = pd.read_sql_table(table, engine)\n",
    "    content_imgs_urls = {}\n",
    "    for row in ori_df.itertuples():\n",
    "        #         print(dir(row))\n",
    "        img_url = {}\n",
    "        img_url['title_img_url'] = row.title_img_url\n",
    "        img_url['title_img_local'] = row.title_img_local\n",
    "        if not content_imgs_urls.get(row.orig_id):\n",
    "            content_imgs_urls[row.orig_id] = img_url\n",
    "\n",
    "    return content_imgs_urls\n",
    "\n",
    "\n",
    "def get_content_imgs_url(table, engine, content_imgs_urls):\n",
    "    ''' get imgs set from new content title\n",
    "    '''\n",
    "    not_down_content = {}\n",
    "    ori_df = pd.read_sql_table(table, engine, columns=['id', 'title', 'new_content'])\n",
    "    ori_df_imgs = ori_df['new_content'].apply(lambda x: BeautifulSoup(x).find_all('img'))\n",
    "    for _id, title, imgs in zip(ori_df['id'].values, ori_df['title'].values, ori_df_imgs.values):\n",
    "        #     print(_id,title,img,type(title),type(img))\n",
    "        img_value = {}\n",
    "        #         title_img_urls = []\n",
    "        content_img_urls = []\n",
    "        for img in imgs:\n",
    "            if isinstance(img, Tag):\n",
    "                content_img_url = img.attrs.get('src')\n",
    "                if re.match(r'http', content_img_url):\n",
    "                    #             print(title_img_url)\n",
    "                    content_img_urls.append(content_img_url)\n",
    "                    if not content_imgs_urls.get(str(_id) + str(title)):\n",
    "                        img_value['title_img_url'] = content_img_urls\n",
    "                        not_down_content[str(_id) + str(title)] = img_value\n",
    "\n",
    "    return not_down_content\n",
    "\n",
    "\n",
    "def get_preview_imgs_url(table, engine, preview_imgs_urls):\n",
    "    ''' get imgs set from preview img links\n",
    "    '''\n",
    "\n",
    "    not_down_previews = {}\n",
    "    ori_df = pd.read_sql_table(table, engine, columns=['id', 'title', 'preview_img_link'])\n",
    "    ori_df_imgs = ori_df['preview_img_link']\n",
    "    for _id, title, img in zip(ori_df['id'].values, ori_df['title'].values, ori_df_imgs.values):\n",
    "        #     print(_id,title,img,type(title),type(img))\n",
    "#         print(imgs)\n",
    "        img_value = {}\n",
    "        preview_img_urls = []\n",
    "#         for img in imgs:\n",
    "#             if re.match(r'http', img):\n",
    "#                 #             print(title_img_url)\n",
    "        preview_img_urls.append(img)\n",
    "        if not preview_imgs_urls.get(str(_id) + str(title)):\n",
    "            img_value['preview_img_link'] = preview_img_urls\n",
    "            not_down_previews[str(_id) + str(title)] = img_value\n",
    "    return not_down_previews\n",
    "\n",
    "\n",
    "def download_imgs(img_urls,img_url_key,img_url_local_key,save_dir,table):\n",
    "    '''download imgs\n",
    "        img_url_key:'preview_img_link'\n",
    "        img_url_local_key:'preview_img_local'\n",
    "    '''\n",
    "    \n",
    "    imgs_downloaded = {}\n",
    "    imgs_not_downloaded = {}\n",
    "    \n",
    "    for ks,vs in img_urls.items():\n",
    "        \n",
    "        for img_key,img_links in vs.items(): #for each item inside \n",
    "            for img_link in img_links:\n",
    "#                 print(img_link)\n",
    "                if img_link: ##if there is link from img element\n",
    "                    if not os.path.exists(os.path.join(save_dir,table)):\n",
    "                        os.mkdir(os.path.join(save_dir,table))\n",
    "                    img_file = os.path.join(preview_save_dir,table,ks+'____'+img_link.split('/')[-1])\n",
    "                    if  not os.path.exists(img_file): #check file already downloaded\n",
    "                        try: ## try to download the img file and save it\n",
    "                            res = req.get(url=img_link,\n",
    "                                          stream=True,\n",
    "                                         headers ={'user-agent':user_agent})\n",
    "                            with open(img_file,'wb') as f:\n",
    "                                f.write(res.content)\n",
    "                            imgs_downloaded[ks] = {img_url_key:img_link,\n",
    "                                                  img_url_local_key:img_file}\n",
    "#                             print('finish downloading')\n",
    "                        except: ## couldn't downloaded it\n",
    "                            continue\n",
    "                            \n",
    "                            imgs_not_downloaded[ks] = {img_url_key:img_link} #save the undownloaded img link\n",
    "                else: ## if img is None,save to avoid searching again\n",
    "                    imgs_not_downloaded[ks] = {img_url_key:None}\n",
    "                    \n",
    "    return imgs_downloaded,imgs_not_downloaded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
